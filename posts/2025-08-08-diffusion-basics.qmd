---
title: "Diffusion model basics"
date: 2025-08-08
---

# Introduction

Diffusion model의 시초는 Sohl-Dickstein의 diffusion probablistic model ([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585)).

이후 Yang Song의 noise-conditioned score network (**NCSN**, a.k.a., **score matching**; [Yang & Ermon, 2019](https://arxiv.org/abs/1907.05600))과 Jonathan Ho의 denoising diffusion probabilistic model (**DDPM**; [Ho et al., 2020](https://arxiv.org/abs/2006.11239)) 부터 그 당시 SOTA 였던 GAN과 comparable한 성능을 보여주기 시작하면서 가능성을 주목받기 시작함.

기본적으로 GAN이나 flow-based model이 그렇듯, 어떤 tractable distribution (e.g., Gaussian)과 data distribution 간의 mapping을 잘 모델링 해서, tractable distribution으로 부터 sampling한 $z$로 부터 data distribution의 $x$를 생성하는 것이 목표임.

Diffusion model은 이러한 mapping을 data distribution의 $x$에 아주 작은 양의 Gaussian noise를 계속 더해서 $x$와 같은 dimension을 가지는 Gaussian random variable로 mapping 하고, 이러한 과정을 역으로 mapping 할 수 있는 function을 학습을 해서 $z$로 부터 $x$를 생성함.

# Denoising Diffusion Probabilistic Model

## Forward Diffusion Process

real data distribution으로 부터 샘플링한 data point $\mathbf{x}_0 \sim q(\mathbf{x})$에 대해 forward diffusion process는 아래와 같의 정의됨

$$ q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}), \quad q(\mathbf{x}_{1:T} | \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t | \mathbf{x}_{t-1}) $$

* Markov chain
* T step 동안 작은 양의 Gaussian noise를 더하는 것으로 정의
* variance schedule $\{\beta_t \in (0, 1) \}_{t=1}^T$에 따라 매 step 마다 Gaussian noise를 더함
* 이에 따라 noisy sample의 sequence $\mathbf{x}_1, \cdots, \mathbf{x}_T$가 정의
* reparameterization trick을 통해 $q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1- \bar{\alpha}_t) \mathbf{I})$로 arbitrary step에 대해 바로 sampling 가능

## Reverse Diffusion Process

DDPM fig2

만약 위에서 정의한 diffusion process를 뒤집어서 $q(\mathbf{x}_{t-1} | \mathbf{x}_t)$에서 샘플링 할 수 있다면, Gaussian noise input $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})로 부터 real data distribution의 data point를 생성할 수 있음.
이러한 reverse process $q(\mathbf{x}_{t-1} | \mathbf{x}_t)$를 쉽게 estimate 할 수는 없지만, noise scale $\beta_t$가 아주 작다는 가정하에, $q(\mathbf{x}_{t-1} | \mathbf{x}_t)$는 Gaussian이고, 따라서 이를 어떤 parameterized model $p_\theta$를 이용해 근사할 수 있음.

$$ p_\theta (\mathbf{x}_{0:T}) := p(\mathbf{x}_T) \prod_{t=1}^T p_\theta (\mathbf{x}_{t-1} | \mathbf{x}_t), \quad p_\theta (\mathbf{x}_{t=1} | \mathbf{x}_t) := \mathcal{N}(\mathbf{x}_{t-1}; \mathbf{\mu}_\theta (\mathbf{x}_t, t), \mathbf{\Sigma}_\theta (\mathbf{x}_t, t)) $$

## Training objective







# Abstract
논문 요약 작성 예시입니다.

## Key Ideas
- 주요 기여점
- 기술적 접근
- 실험 결과

수식 예시:

$$
\mathcal{L} = \sum_i (y_i - \hat{y}_i)^2
$$

```python
# 예시 코드 블록
import torch
print(torch.__version__)
