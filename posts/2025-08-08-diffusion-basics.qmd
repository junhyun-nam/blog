---
title: "Diffusion model basics"
date: 2025-08-08
---

# Introduction

Diffusion model의 시초는 Sohl-Dickstein의 diffusion probablistic model ([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585)).

이후 Yang Song의 noise-conditioned score network (**NCSN**, a.k.a., **score matching**; [Yang & Ermon, 2019](https://arxiv.org/abs/1907.05600))과 Jonathan Ho의 denoising diffusion probabilistic model (**DDPM**; [Ho et al., 2020](https://arxiv.org/abs/2006.11239)) 부터 그 당시 SOTA 였던 GAN과 comparable한 성능을 보여주기 시작하면서 가능성을 주목받기 시작함.

기본적으로 GAN이나 flow-based model이 그렇듯, 어떤 tractable distribution (e.g., Gaussian)과 data distribution 간의 mapping을 잘 모델링 해서, tractable distribution으로 부터 sampling한 $z$로 부터 data distribution의 $x$를 생성하는 것이 목표임.

Diffusion model은 이러한 mapping을 data distribution의 $x$에 아주 작은 양의 Gaussian noise를 계속 더해서 $x$와 같은 dimension을 가지는 Gaussian random variable로 mapping 하고, 이러한 과정을 역으로 mapping 할 수 있는 function을 학습을 해서 $z$로 부터 $x$를 생성함.

# Denoising Diffusion Probabilistic Model

## Forward Diffusion Process

real data distribution으로 부터 샘플링한 data point $\mathbf{x}_0 \sim q(\mathbf{x})$에 대해 forward diffusion process는 아래와 같의 정의됨

$$ q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}), \quad q(\mathbf{x}_{1:T} | \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t | \mathbf{x}_{t-1}) $$

* Markov chain 임
* T step 동안 작은 양의 Gaussian noise를 더하는 것으로 정의
* 이에 따라 noisy sample의 sequence $\mathbf{x}_1, \cdots, \mathbf{x}_T$가 정의
* 매 step 마다 더해지는 Gaussian noise의 양은 variance schedule $\{\beta_t \in (0, 1) \}_{t=1}^T$
* reparameterization trick을 통해 $q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1- \bar{\alpha}_t) \mathbf{I})로 arbitrary step에 대해 바로 sampling 가능




# Abstract
논문 요약 작성 예시입니다.

## Key Ideas
- 주요 기여점
- 기술적 접근
- 실험 결과

수식 예시:

$$
\mathcal{L} = \sum_i (y_i - \hat{y}_i)^2
$$

```python
# 예시 코드 블록
import torch
print(torch.__version__)
