[
  {
    "objectID": "posts/2025-08-08-first-post.html",
    "href": "posts/2025-08-08-first-post.html",
    "title": "My First Paper Summary",
    "section": "",
    "text": "논문 요약 작성 예시입니다.\n\n\n\n주요 기여점\n기술적 접근\n실험 결과\n\n수식 예시:\n\\[\n\\mathcal{L} = \\sum_i (y_i - \\hat{y}_i)^2\n\\]\n```python # 예시 코드 블록 import torch print(torch.__version__)"
  },
  {
    "objectID": "posts/2025-08-08-first-post.html#key-ideas",
    "href": "posts/2025-08-08-first-post.html#key-ideas",
    "title": "My First Paper Summary",
    "section": "",
    "text": "주요 기여점\n기술적 접근\n실험 결과\n\n수식 예시:\n\\[\n\\mathcal{L} = \\sum_i (y_i - \\hat{y}_i)^2\n\\]\n```python # 예시 코드 블록 import torch print(torch.__version__)"
  },
  {
    "objectID": "posts/2025-08-08-diffusion-basics.html",
    "href": "posts/2025-08-08-diffusion-basics.html",
    "title": "Diffusion model basics",
    "section": "",
    "text": "Diffusion model의 시초는 Sohl-Dickstein의 diffusion probablistic model (Sohl-Dickstein et al., 2015).\n이후 Yang Song의 noise-conditioned score network (NCSN, a.k.a., score matching; Yang & Ermon, 2019)과 Jonathan Ho의 denoising diffusion probabilistic model (DDPM; Ho et al., 2020) 부터 그 당시 SOTA 였던 GAN과 comparable한 성능을 보여주기 시작하면서 가능성을 주목받기 시작함.\n기본적으로 GAN이나 flow-based model이 그렇듯, 어떤 tractable distribution (e.g., Gaussian)과 data distribution 간의 mapping을 잘 모델링 해서, tractable distribution으로 부터 sampling한 \\(z\\)로 부터 data distribution의 \\(x\\)를 생성하는 것이 목표임.\nDiffusion model은 이러한 mapping을 data distribution의 \\(x\\)에 아주 작은 양의 Gaussian noise를 계속 더해서 \\(x\\)와 같은 dimension을 가지는 Gaussian random variable로 mapping 하고, 이러한 과정을 역으로 mapping 할 수 있는 function을 학습을 해서 \\(z\\)로 부터 \\(x\\)를 생성함."
  },
  {
    "objectID": "posts/2025-08-08-diffusion-basics.html#forward-diffusion-process",
    "href": "posts/2025-08-08-diffusion-basics.html#forward-diffusion-process",
    "title": "Diffusion model basics",
    "section": "Forward Diffusion Process",
    "text": "Forward Diffusion Process\nreal data distribution으로 부터 샘플링한 data point \\(\\mathbf{x}_0 \\sim q(\\mathbf{x})\\)에 대해 forward diffusion process는 아래와 같의 정의됨\n\\[ q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}), \\quad q(\\mathbf{x}_{1:T} | \\mathbf{x}_0) = \\prod_{t=1}^T q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) \\]\n\nMarkov chain\nT step 동안 작은 양의 Gaussian noise를 더하는 것으로 정의\nvariance schedule \\(\\{\\beta_t \\in (0, 1) \\}_{t=1}^T\\)에 따라 매 step 마다 Gaussian noise를 더함\n이에 따라 noisy sample의 sequence \\(\\mathbf{x}_1, \\cdots, \\mathbf{x}_T\\)가 정의\nreparameterization trick을 통해 \\(q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1- \\bar{\\alpha}_t) \\mathbf{I})\\)로 arbitrary step에 대해 바로 sampling 가능"
  },
  {
    "objectID": "posts/2025-08-08-diffusion-basics.html#reverse-diffusion-process",
    "href": "posts/2025-08-08-diffusion-basics.html#reverse-diffusion-process",
    "title": "Diffusion model basics",
    "section": "Reverse Diffusion Process",
    "text": "Reverse Diffusion Process\nDDPM fig2\n만약 위에서 정의한 diffusion process를 뒤집어서 \\(q(\\mathbf{x}_{t-1} | \\mathbf{x}_t)\\)에서 샘플링 할 수 있다면, Gaussian noise input \\(\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\)로 부터 real data distribution의 data point를 생성할 수 있음.\n이러한 reverse process \\(q(\\mathbf{x}_{t-1} | \\mathbf{x}_t)\\)를 쉽게 estimate 할 수는 없지만, noise scale \\(\\beta_t\\)가 아주 작다는 가정하에, \\(q(\\mathbf{x}_{t-1} | \\mathbf{x}_t)\\)는 Gaussian이고, 따라서 이를 어떤 parameterized model \\(p_\\theta\\)를 이용해 근사할 수 있음.\n\\[ p_\\theta (\\mathbf{x}_{0:T}) := p(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_t), \\quad p_\\theta (\\mathbf{x}_{t=1} | \\mathbf{x}_t) := \\mathcal{N}(\\mathbf{x}_{t-1}; \\mathbf{\\mu}_\\theta (\\mathbf{x}_t, t), \\mathbf{\\Sigma}_\\theta (\\mathbf{x}_t, t)) \\]"
  },
  {
    "objectID": "posts/2025-08-08-diffusion-basics.html#training-objective",
    "href": "posts/2025-08-08-diffusion-basics.html#training-objective",
    "title": "Diffusion model basics",
    "section": "Training objective",
    "text": "Training objective"
  },
  {
    "objectID": "posts/2025-08-08-diffusion-basics.html#key-ideas",
    "href": "posts/2025-08-08-diffusion-basics.html#key-ideas",
    "title": "Diffusion model basics",
    "section": "Key Ideas",
    "text": "Key Ideas\n\n주요 기여점\n기술적 접근\n실험 결과\n\n수식 예시:\n\\[\n\\mathcal{L} = \\sum_i (y_i - \\hat{y}_i)^2\n\\]\n```python # 예시 코드 블록 import torch print(torch.__version__)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "이 블로그는 아이패드에서도 바로 편집 가능하도록\nGitHub Codespaces와 Quarto를 사용해 제작되었습니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal ML Paper Study",
    "section": "",
    "text": "Diffusion Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiffusion model basics\n\n\n\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMy First Paper Summary\n\n\n\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/category-diffusion.html",
    "href": "posts/category-diffusion.html",
    "title": "Diffusion Models",
    "section": "",
    "text": "여기는 Diffusion Models 분야 논문 정리 페이지입니다.\n\nBasics",
    "crumbs": [
      "Home",
      "Topics",
      "Diffusion Models"
    ]
  }
]